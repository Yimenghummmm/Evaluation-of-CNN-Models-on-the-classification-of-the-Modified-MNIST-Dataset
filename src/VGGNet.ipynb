{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n# from tensorflow.kears import layers, models, datasets\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam, SGD\nimport torch.nn.functional as F\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# for creating validation set\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom torchvision import transforms\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_images = pd.read_pickle('/kaggle/input/modified-mnist/train_max_x') \ntest_images = pd.read_pickle('/kaggle/input/modified-mnist/test_max_x')\ntrain_label = pd.read_csv('/kaggle/input/modified-mnist/train_max_y.csv')\ntrain_label = train_label['Label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def filter_image(img):\n#     image = np.array(img, dtype=np.uint8)\n#     image = (255-image)\n\n    \n#     gaus = cv2.GaussianBlur(image, (1,1), 0)\n\n    \n#     median = cv2.medianBlur(gaus, 1)\n\n    \n#     thresh = cv2.threshold(median.copy(), 35, 255, cv2.THRESH_BINARY)[1]\n\n#     return thresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filter \n# for i in range (0, len(train_images)):\n    \n#     train_images[i] = filter_image(train_images[i])\n    \n# for i in range (0, len(test_images)):\n#     test_images[i] = filter_image(test_images[i])\n\n# for i in range (0, 10):\n#     plt.imshow(train_images[i], 'gray', vmin=0, vmax=255)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(train_images, train_label, test_size = 0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train/255.0\nx_test = x_test/255.0\nx_train = x_train.reshape(40000,1,128,128)\nx_test = x_test.reshape(10000,1,128,128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.array(x_train)\ny_train = np.array(y_train)\n\nx_test = np.array(x_test)\ny_test = np.array(y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build the data loader\nimport torch.utils.data as utils\n\ntensor_x_train  = torch.from_numpy(x_train)\ntensor_y_train = torch.from_numpy(y_train)\ntrain_data = utils.TensorDataset(tensor_x_train,tensor_y_train) # create your datset\n\ntensor_x_test  = torch.from_numpy(x_test)\ntensor_y_test = torch.from_numpy(y_test)\ntest_data = utils.TensorDataset(tensor_x_test,tensor_y_test) # create your datset\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = utils.DataLoader(train_data,batch_size = 16) # create your dataloader\ntest_loader = utils.DataLoader(test_data,batch_size = 16) # create your dataloader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x_train, x_test, y_train, y_test \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_images, train_label, test_images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# from torchvision import models\n# import torch\nimport torchvision.models as models\nvgg13 = models.vgg13()\nvgg11 = models.vgg11()\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(vgg13)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class VGGNET13(nn.Module):    \n    def __init__(self):\n        super(VGGNET13, self).__init__()\n          \n        self.features = nn.Sequential(  \n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3,stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=512*4*4, out_features=1024, bias=True),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=1024, out_features=216, bias=True),\n            nn.BatchNorm1d(216),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=216, out_features=10, bias=True)\n        )\n          \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VGGNET16(nn.Module):    \n    def __init__(self):\n        super(VGGNET16, self).__init__()        \n        self.features = nn.Sequential(  \n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3,stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=512*4*4, out_features=1024, bias=True),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=1024, out_features=216, bias=True),\n            nn.BatchNorm1d(216),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=216, out_features=10, bias=True)\n        )\n        \n          \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"class VGGNET19(nn.Module):    \n    def __init__(self):\n        super(VGGNET19, self).__init__()\n          \n        self.features = nn.Sequential(  \n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3,stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=512*4*4, out_features=1024, bias=True),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=1024, out_features=216, bias=True),\n            nn.BatchNorm1d(216),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=216, out_features=10, bias=True)\n        )\n          \n#         for m in self.features.children():\n#             if isinstance(m, nn.Conv2d):\n#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n#             elif isinstance(m, nn.BatchNorm2d):\n#                 m.weight.data.fill_(1)\n#                 m.bias.data.zero_()\n        \n#         for m in self.classifier.children():\n#             if isinstance(m, nn.Linear):\n#                 nn.init.xavier_uniform(m.weight)\n#             elif isinstance(m, nn.BatchNorm1d):\n#                 m.weight.data.fill_(1)\n#                 m.bias.data.zero_()\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x     "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nmodel = VGGNET16()\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss()\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch):\n    model.train()\n    \n    train_loss_all = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n    \n        train_loss_all += F.cross_entropy(output, target, size_average=False).item()\n        train_loss_all /= len(train_loader.dataset)\n        loss.backward()\n        optimizer.step()\n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n    train_loss.append(train_loss_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(data_loader):\n    model.eval()\n    loss = 0\n    correct = 0\n    with torch.no_grad():    \n        for data, target in data_loader:\n            data, target = Variable(data, volatile=True), Variable(target)\n            if torch.cuda.is_available():\n                data = data.cuda()\n                target = target.cuda()\n\n            output = model(data)\n\n            loss += F.cross_entropy(output, target, size_average=False).item()\n\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n        loss /= len(data_loader.dataset)\n        validation_loss.append(loss)\n\n        print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n            loss, correct, len(data_loader.dataset),\n            100. * correct / len(data_loader.dataset)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = []\nvalidation_loss = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 20\nfor epoch in range(n_epochs):\n    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n    train(epoch)\n    evaluate(test_loader)\n    exp_lr_scheduler.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = test_images/255.0\ntest_images = test_images.reshape(10000,1,128,128)\ntest_images = np.array(test_images)\ntensor_test_images = torch.from_numpy(test_images)\npredict_loader = utils.DataLoader(tensor_test_images,batch_size = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_df = pd.DataFrame(np.c_[train_loss, validation_loss], \n                      columns=['TrainLoss', 'ValidationLoss'])\nout_df.to_csv('loss2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = {'model': VGGNET16(),\n              'state_dict': model.state_dict(),\n              'optimizer' : optimizer.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint_withoucleaning.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediciton(data_loader):\n    model.eval()\n    test_pred = torch.LongTensor()\n    \n    for i, data in enumerate(data_loader):\n        data = Variable(data, volatile=True)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            \n        output = model(data)\n        \n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n        \n    return test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = prediciton(predict_loader)\nout_df = pd.DataFrame(np.c_[np.arange(0, len(test_images))[:,None], pred.numpy()], \n                      columns=['Id', 'Label'])\nout_df.to_csv('submission_VGG1620epochsnocleaning.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}